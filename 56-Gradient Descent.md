**Gradient Descent**  An iterative optimization algorithm used to find a local minimum of a differentiable function  
- Works well on convex function which has a single minimum

**Types**
- Batch
  -  Best for small datasets but can be slow and memory-intensive for large datasets.
- Stochastic
  - Ideal for large datasets and online learning, but updates are noisy and may oscillate. 
- Mini
  - Combines the advantages of both BGD and SGD, providing a good balance between convergence speed and stability, and is often the most commonly used in practice for large datasets  
 
 ![Screenshot 2024-12-29 at 1 36 39â€¯PM](https://github.com/user-attachments/assets/d85c07c5-8da3-4f28-bdce-5280b980c675)

