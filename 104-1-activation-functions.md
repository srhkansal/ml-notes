**Why activation function**  
In a perceptron, the purpose of applying an activation function to the linear combination of inputs (the weighted sum) is to introduce non-linearity into the model.  

![Screenshot 2024-12-23 at 10 24 40â€¯PM](https://github.com/user-attachments/assets/dd64c41a-3894-45c5-9879-94ab256af735)  






**applying an activation function to the linear output in a perceptron is essential for:**   
- Introducing non-linearity: Making the perceptron capable of solving non-linear problems and creating non-linear decision boundaries.  
- Enabling complex learning: Allowing the perceptron (and more complex neural networks) to learn intricate patterns in the data.  
- Ensuring gradient flow: Allowing for effective weight updates during training via backpropagation.  
- Making discrete predictions: Enabling the perceptron to make binary decisions, like classifying data into two categories.  
